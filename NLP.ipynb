{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbwLSTDOGpAGEgPenb3rta",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Snair910/Google-Data-Analytics-Capstone/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SnSrhUrtGus",
        "outputId": "1248eb3d-84b8-4951-d939-718399093454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']], [['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer\n",
        "import string\n",
        "import math\n",
        "\n",
        "# Loading Brown Corpus paragraphs\n",
        "brown_paragraphs = brown.paras()\n",
        "\n",
        "# NLTK resources download\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "print(brown_paragraphs[:2])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Remove stopwords from each paragraph\n",
        "filtered_paragraphs = []\n",
        "for paragraph in brown_paragraphs:\n",
        "    filtered_tokens = [word for sent in paragraph for word in sent if word.lower() not in stopwords.words('english')]\n",
        "    filtered_paragraphs.append(filtered_tokens)\n",
        "\n",
        "# Print the first 2 paragraphs with stopwords removed\n",
        "for i, paragraph in enumerate(filtered_paragraphs[:2]):\n",
        "    print(f\"Paragraph {i+1}: {paragraph}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKVIE_eSChZC",
        "outputId": "03de97e1-c500-4d06-dd8c-75e26d6c5bf7"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 1: ['Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'investigation', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'evidence', \"''\", 'irregularities', 'took', 'place', '.']\n",
            "Paragraph 2: ['jury', 'said', 'term-end', 'presentments', 'City', 'Executive', 'Committee', ',', 'over-all', 'charge', 'election', ',', '``', 'deserves', 'praise', 'thanks', 'City', 'Atlanta', \"''\", 'manner', 'election', 'conducted', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "#stopwords and punctuation\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "# Process paragraphs: Removing punctuation and tokenizing into words\n",
        "filtered_paragraphs = []\n",
        "\n",
        "for paragraph in brown_paragraphs:\n",
        "    filtered_sentence = []\n",
        "    for sentence in paragraph:\n",
        "        text = ' '.join(sentence).lower()\n",
        "        filtered_text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        words = word_tokenize(filtered_text)\n",
        "        filtered_sentence.extend(words)\n",
        "    filtered_paragraphs.append(filtered_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOFGoCtCtNdg",
        "outputId": "92cde503-f3f1-4c0f-eba8-ca4c7cc2aa87"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47415\n",
            "[\"''\", \"'13\", \"'20\", \"'20s\", \"'25\", \"'30s\", \"'31\", \"'38\", \"'40\", \"'48\"]\n",
            "['900', '900,000', '900-calorie', '900-student', '91', '92', '92.5', '920', '923,076', '9230']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Apply stemming to filtered tokens\n",
        "lancaster = LancasterStemmer()\n",
        "stemmed_tokens = [lancaster.stem(token) for token in filtered_tokens]\n",
        "# Display the original and stemmed words (first 10 for demonstration)\n",
        "print(\"Original words:\", filtered_tokens[:10])\n",
        "print(\"Stemmed words with Lancaster Stemmer:\", stemmed_tokens[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsfqoYmW3XzG",
        "outputId": "b6645964-1af4-42d8-b766-08d055c5a6c1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['fulton', 'county', 'grand', 'jury', 'said', 'friday', 'investigation', 'atlanta', \"'s\", 'recent']\n",
            "Stemmed words with Lancaster Stemmer: ['fulton', 'county', 'grand', 'jury', 'said', 'friday', 'investig', 'atlant', \"'s\", 'rec']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Term Frequencies (TF) for stemmed tokens\n",
        "tf = nltk.FreqDist(stemmed_tokens)\n",
        "\n",
        "# Display the top 10 words and their TF values after preprocessing and stemming\n",
        "top_10_tf = tf.most_common(10)\n",
        "for word, frequency in top_10_tf:\n",
        "    print(f\"Word: {word}, TF: {frequency}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdQ-votzzKc8",
        "outputId": "8b7b12d7-c988-4d1e-bd3c-e50d21f5858f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: ``, TF: 17537\n",
            "Word: 's, TF: 5865\n",
            "Word: on, TF: 3496\n",
            "Word: --, TF: 3435\n",
            "Word: would, TF: 2844\n",
            "Word: us, TF: 2490\n",
            "Word: stat, TF: 2151\n",
            "Word: n't, TF: 2100\n",
            "Word: tim, TF: 1962\n",
            "Word: said, TF: 1961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Inverse Document Frequency (IDF)\n",
        "doc_size = 1000\n",
        "ntokens = len(stemmed_tokens)\n",
        "ndocs = math.ceil(ntokens / doc_size)\n",
        "\n",
        "tf_idf = {}\n",
        "\n",
        "for token in set(stemmed_tokens):\n",
        "    count = sum(1 for doc in brown_paragraphs if token in ' '.join(doc))\n",
        "    tf_idf[token] = tf[token] * math.log(ndocs / count)\n",
        "\n",
        "# Display top 10 words and their TF-IDF values\n",
        "sorted_tf_idf = sorted(tf_idf.items(), key=operator.itemgetter(1), reverse=True)[:10]\n",
        "for word, tfidf in sorted_tf_idf:\n",
        "    print(f\"Word: {word}, TF-IDF: {tfidf}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "pv92-jS44hvu",
        "outputId": "07beec0a-804c-40ca-f1fa-8dc07df5b12c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-449fdcac15c9>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbrown_paragraphs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtf_idf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndocs\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-449fdcac15c9>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbrown_paragraphs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtf_idf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndocs\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "# POS tagging\n",
        "tags = nltk.pos_tag(filtered_tokens)\n",
        "print(tags[:20])  # Displaying the first 20 tagged tokens\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxUi3Wml-dBd",
        "outputId": "1afdefe5-ac31-40c2-a62c-71a4936427ef"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('fulton', 'NN'), ('county', 'NN'), ('grand', 'JJ'), ('jury', 'NN'), ('said', 'VBD'), ('friday', 'JJ'), ('investigation', 'NN'), ('atlanta', 'NN'), (\"'s\", 'POS'), ('recent', 'JJ'), ('primary', 'JJ'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('evidence', 'NN'), ('``', '``'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('jury', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Obtain the frequency distribution of the tags\n",
        "tf_tag = nltk.FreqDist(tag for (word, tag) in tags)\n",
        "print(tf_tag.most_common(10))  # Displaying the top 10 POS tags and their frequencies\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6Cv9PThANQO",
        "outputId": "59bdeba4-f557-44fd-8c6d-6388dac4e4b9"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('NN', 176976), ('JJ', 108638), ('NNS', 59727), ('RB', 35806), ('VBD', 34694), ('VBP', 20472), ('VBG', 19432), ('``', 17537), ('VBN', 15583), ('CD', 15365)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Most common nouns\n",
        "tf_word_tag = nltk.FreqDist(tags)\n",
        "most_common_nouns = [word for ((word, tag), _) in tf_word_tag.most_common() if tag.startswith('NN')][:20]\n",
        "print(most_common_nouns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rWVCmG6AXaS",
        "outputId": "eba832fa-b6f7-4845-e048-78a3afdc4b99"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['time', 'man', 'years', 'way', 'people', 'state', 'world', 'men', 'life', 'day', 'year', 'work', 'house', 'states', 'place', 'part', 'home', 'school', 'number', 'war']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Most common verbs using regular expression\n",
        "most_common_verbs = [wt[0] for (wt, _) in tf_word_tag.most_common() if re.search('^VB.*$', wt[1])][:20]\n",
        "print(most_common_verbs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J31YEW_TAOYH",
        "outputId": "61a2927a-88e7-4bb8-c5bc-b0eacfbb5431"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['said', 'made', 'came', 'get', 'went', 'make', 'used', 'take', 'took', 'made', 'going', 'got', 'go', 'given', 'left', 'say', 'found', 'see', 'looked', 'asked']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Most common word pairs (bigrams)\n",
        "tf_word_tag_pair = nltk.FreqDist(nltk.bigrams(tags))\n",
        "most_common_word_pairs = tf_word_tag_pair.most_common(20)\n",
        "print(most_common_word_pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMyl_XoAAbMJ",
        "outputId": "dbcadfc8-31c1-4e9b-b715-a932b5f7bc49"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[((('``', '``'), ('``', '``')), 1610), ((('said', 'VBD'), ('``', '``')), 661), ((('``', '``'), ('said', 'VBD')), 562), ((('united', 'JJ'), ('states', 'NNS')), 328), ((('new', 'JJ'), ('york', 'NN')), 280), ((('``', '``'), (\"n't\", 'RB')), 276), ((('``', '``'), (\"'s\", 'POS')), 247), ((('``', '``'), ('--', ':')), 201), ((('could', 'MD'), (\"n't\", 'RB')), 176), ((('ca', 'MD'), (\"n't\", 'RB')), 169), ((('per', 'IN'), ('cent', 'NN')), 146), ((('would', 'MD'), (\"n't\", 'RB')), 130), ((('man', 'NN'), (\"'s\", 'POS')), 128), ((('years', 'NNS'), ('ago', 'RB')), 124), ((('--', ':'), ('``', '``')), 118), ((('``', '``'), (\"'ll\", 'MD')), 118), (((\"'s\", 'POS'), ('``', '``')), 113), ((('wo', 'MD'), (\"n't\", 'RB')), 105), ((('``', '``'), ('one', 'CD')), 104), (((\"n't\", 'RB'), ('know', 'VB')), 104)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find most common trigrams of word-tag pairs\n",
        "trigrams = nltk.trigrams(tags)\n",
        "freq_dist_trigrams = nltk.FreqDist(trigrams)\n",
        "common_trigrams = freq_dist_trigrams.most_common(10)\n",
        "\n",
        "# Display the 10 most common trigrams of word-tag pairs and their frequencies\n",
        "for trigram, freq in common_trigrams:\n",
        "    print(f\"Trigram: {trigram}, Frequency: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDbehxzDBmh4",
        "outputId": "08c2c372-b9f0-48b1-cccf-f56966825447"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trigram: (('``', '``'), ('said', 'VBD'), ('``', '``')), Frequency: 200\n",
            "Trigram: (('``', '``'), ('``', '``'), ('``', '``')), Frequency: 97\n",
            "Trigram: (('``', '``'), ('``', '``'), (\"n't\", 'RB')), Frequency: 72\n",
            "Trigram: (('``', '``'), ('--', ':'), ('``', '``')), Frequency: 52\n",
            "Trigram: (('``', '``'), ('ca', 'MD'), (\"n't\", 'RB')), Frequency: 47\n",
            "Trigram: (('``', '``'), ('``', '``'), (\"'s\", 'POS')), Frequency: 44\n",
            "Trigram: (('``', '``'), ('``', '``'), ('said', 'VBD')), Frequency: 38\n",
            "Trigram: (('world', 'NN'), ('war', 'NN'), ('2', 'CD')), Frequency: 35\n",
            "Trigram: (('``', '``'), (\"n't\", 'RB'), ('know', 'VB')), Frequency: 35\n",
            "Trigram: (('said', 'VBD'), ('``', '``'), (\"'s\", 'POS')), Frequency: 33\n"
          ]
        }
      ]
    }
  ]
}